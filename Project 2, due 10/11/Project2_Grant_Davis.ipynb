{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ncDfQIZ5gG"
      },
      "source": [
        "# **Project 2 Report**\n",
        "\n",
        "Grant Davis\n",
        "\n",
        "CS458"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8goBinPaYnd"
      },
      "source": [
        "## **P2-1. Decision Tree**\n",
        "\n",
        "**(a) Develop a decision tree based classifier to classify the 3 different types  of Iris (Setosa, Versicolour, and Virginica).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4HJGE1w1aTqC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets, model_selection, tree, metrics, svm, preprocessing\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "iris = datasets.load_iris()\n",
        "clf = tree.DecisionTreeClassifier(random_state=0)\n",
        "clf = clf.fit(iris.data, iris.target)\n",
        "tree.plot_tree(clf, filled=True, class_names=iris.target_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onIeaqM3a_2C"
      },
      "source": [
        "Using support vector classification to classify Iris types; Setosa, Versicolour, and Virginica. Every run cross validation score is recorded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y697pjx7bEqB"
      },
      "source": [
        "**(b) Optimize the parameters of your decision tree to maximize the classification accuracy. Show the confusion matrix of your decision tree. Plot your decision tree.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VGfsolCrbInO",
        "outputId": "6ad9aa14-3a19-4410-d01d-801cf0297e49",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Print your classification accuracy, confusion matrix and plot your decision tree.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets, model_selection, tree, metrics, svm, preprocessing\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "clf = tree.DecisionTreeClassifier(random_state=0)\n",
        "clf = clf.fit(iris.data, iris.target)\n",
        "\n",
        "trainX, testX, trainY, testY = model_selection.train_test_split(\n",
        "  iris.data, iris.target, test_size=0.1, random_state=0)\n",
        "\n",
        "avgAccuracy = 0.0\n",
        "skf = model_selection.StratifiedKFold(n_splits=5)\n",
        "skf.get_n_splits(iris.data, iris.target)\n",
        "for train_index, test_index in skf.split(iris.data, iris.target):\n",
        "  trainX, testX = iris.data[train_index], iris.data[test_index]\n",
        "  trainY, testY = iris.target[train_index], iris.target[test_index]\n",
        "  clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
        "  scores = model_selection.cross_val_score(clf, trainX, trainY, cv=5)\n",
        "  avgAccuracy += np.average(scores)\n",
        "avgAccuracy /= 5\n",
        "\n",
        "print(\"Accuracy of five-fold cross validation: \", avgAccuracy)\n",
        "\n",
        "max_depth_range = [None, 2, 5, 10]\n",
        "min_samples_leaf_range = [1, 5, 10]\n",
        "min_sample_split_range = [2, 10, 20]\n",
        "min_leaf_nodes_range = [None, 5, 10, 20]\n",
        "\n",
        "param_grid = {\n",
        "  \"criterion\": ['gini'],\n",
        "  \"max_depth\": max_depth_range,\n",
        "  \"min_samples_leaf\": min_samples_leaf_range,\n",
        "  \"min_samples_split\": min_sample_split_range,\n",
        "  \"max_leaf_nodes\": min_leaf_nodes_range\n",
        "}\n",
        "\n",
        "grid = model_selection.GridSearchCV(estimator=tree.DecisionTreeClassifier(),\n",
        "                                    param_grid=param_grid,\n",
        "                                    cv=5,\n",
        "                                    scoring='accuracy',\n",
        "                                    refit=True)\n",
        "\n",
        "clf = make_pipeline(preprocessing.StandardScaler(), grid)\n",
        "clf.fit(trainX, trainY)\n",
        "print(\"Accuracy of hyperparameter tuning: \", grid.best_score_)\n",
        "print(grid.best_params_)\n",
        "y_pred = grid.best_estimator_.predict(testX)\n",
        "print(metrics.confusion_matrix(testY, y_pred))\n",
        "tree.plot_tree(grid.best_estimator_,\n",
        "               filled=True,\n",
        "               class_names=iris.target_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![<caption>](p2-1-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy of five-fold cross validation: 0.978333333333333\n",
        "\n",
        "Accuracy of hyperparameter tuning: 0.9666666666666\n",
        "\n",
        "{'criterion': 'gini','max_depth': None, 'max_leaf_nodes':5,'min_samples_leaf':1,'min_samples_split':20}\n",
        "\n",
        "[[0 0 10]\n",
        "\n",
        " [0 0 10]\n",
        " \n",
        " [0 0 10]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NqsEu79cftE"
      },
      "source": [
        "The ranges of the parameters were based were randomly chosen values that were adjusted as needed. For each values tested the grid search cross validation returns back with best results. The tree shown is the best to predict classes for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vusqyELcnd5"
      },
      "source": [
        "# **P2-2. Model Overfitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbKeiSBCcx64"
      },
      "source": [
        "**(a) Generate the dataset as in slide 56 in Chapter 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m70sWNEMc0a8"
      },
      "source": [
        "**(b) Randomly select 10% of the data as test dataset and the remaining 90% of the data as training dataset. Train decision trees by increasing the number of nodes of the decision trees until the training error becomes 0. Plot the training errors and the testing errors under different numbers of nodes and explain the model underfitting and model overfitting.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F5PVO7WkdCAh",
        "outputId": "f2c32dae-0b60-4c19-d490-c29d47f08444",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plot the training errors and the testing errors under different numbers of nodes\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection, tree, metrics\n",
        "\n",
        "# Generate dataset\n",
        "## 5000 instances (Gaussian)\n",
        "gaus_center = np.random.normal(loc=np.array([10,10]), scale=np.sqrt(2), size=(5000,2))\n",
        "### 200 instances (Uniform)\n",
        "gaus_noise = np.random.uniform(low=0, high=20, size=(200,2))\n",
        "c1 = np.concatenate((gaus_center, gaus_noise), axis=0)\n",
        "\n",
        "## 5200 instances (Uniform)\n",
        "c2 = np.random.uniform(low=0, high=20, size=(5200,2))\n",
        "\n",
        "plt.scatter(c2[:, 0], c2[:, 1], c='red', marker='.', s=2.5)\n",
        "plt.scatter(c1[:, 0], c1[:, 1], c='blue', marker='+', s=2.5)\n",
        "\n",
        "fig, axs = plt.subplots(1,2)\n",
        "\n",
        "c3 = np.concatenate((c1,c2), axis=0)\n",
        "c3_target = np.concatenate((np.zeros((c1.shape[0],1)), np.ones((c2.shape[0],1))), axis=0)\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(c3, c3_target, test_size=0.1, random_state=0, shuffle=True)\n",
        "\n",
        "TrainError = np.empty((0,2))\n",
        "TestError = np.empty((0,2))\n",
        "for nodes in range(2, 151):\n",
        "  clf = tree.DecisionTreeClassifier(max_leaf_nodes=nodes)\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred_train = clf.predict(X_train)\n",
        "  y_pred_test = clf.predict(X_test)\n",
        "  TrainError = np.append(TrainError, np.array([(nodes, 1-metrics.accuracy_score(y_train, y_pred_train))]), axis=0)\n",
        "  TestError = np.append(TestError, np.array([(nodes, 1-metrics.accuracy_score(y_test, y_pred_test))]), axis=0)\n",
        "\n",
        "axs[0].plot(TrainError[:9, 0], TrainError[:9, 1], c='blue', marker='o', markersize=2)\n",
        "axs[0].plot(TestError[:9, 0], TestError[:9, 1], c='red', marker='o', markersize=2)\n",
        "axs[1].plot(TrainError[:, 0], TrainError[:, 1], c='blue', marker='o', markersize=2)\n",
        "axs[1].plot(TestError[:, 0], TestError[:, 1], c='red', marker='o', markersize=2)\n",
        "\n",
        "axs[1].set_xlabel(\"Number of nodes\")\n",
        "axs[0].set_ylabel(\"Error rate\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![<caption>](p2-2-1.png)\n",
        "![<caption>](p2-2-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_5NEhCBdXoH"
      },
      "source": [
        "With a small number of nodes the model does not preform well. The model is tuned too closely to the training data and will overfit with a larger number of nodes. The model cannot generalize itself to the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot87gIvndb4b"
      },
      "source": [
        "# **P2-3. Text Documents Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcX7qMj9dfLm"
      },
      "source": [
        "**(a) Load the following 4 categories from the 20 newsgroups dataset: categories = ['rec.autos', 'talk.religion.misc', 'comp.graphics', 'sci.space']. Print the number of documents in the training dataset and the test dataset. Print the number of attributes in the training dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pix10eIdvcX"
      },
      "source": [
        "**(b) Optimize the parameters of your decision tree to maximize the classification accuracy. Show the confusion matrix of your decision tree.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9AzoG0W3d4y_",
        "outputId": "4e6a6c07-b975-4d9a-93ed-eca479945c00",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Print your classification accuracy, confusion matrix.\n"
          ]
        }
      ],
      "source": [
        "from os import pipe\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, tree, model_selection, metrics, preprocessing, pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, Tfidfvectorize\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "#load newsgroups\n",
        "ng_train = datasets.fetch_20newsgroups(subset='train', categories=['rec.autos', 'talk.religion.misc', 'comp.graphics', 'sci.space'], remove=('headers', 'footers', 'quotes'))\n",
        "ng_test = datasets.fetch_20newsgroups(subset='test', categories=['rec.autos', 'talk.religion.misc', 'comp.graphics', 'sci.space'], remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "print(f'''\n",
        "Set\\t_|_ # Docs\\t_|_ Attributes''')\n",
        "\n",
        "#decision tree\n",
        "max_depth_range = [None, 2, 5, 10]\n",
        "min_samples_leaf_range = [1, 5, 10]\n",
        "min_sample_split_range = [2, 10, 20]\n",
        "min_leaf_nodes_range = [None, 5, 10, 20]\n",
        "\n",
        "pipe_ = Pipeline([('vect', Tfidfvectorize()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', tree.DecisionTreeClassifier())])\n",
        "\n",
        "\n",
        "param_grid = {\"clf__criterion\": ['gini'],\n",
        "              \"clf__max_depth\": [10],\n",
        "              \"clf__min_samples_leaf\": [1, 5, 10],\n",
        "              \"clf__min_samples_split\": [20],\n",
        "              \"clf__max_leaf_nodes\": [None, 5, 10, 20]\n",
        "              }\n",
        "grid = model_selection.GridSearchCV(estimator=pipe_, param_grid=param_grid, scoring='accuracy', refit=True, verbose=True)\n",
        "\n",
        "vectorize = Tfidfvectorize(sublinear_tf=True, max_df=0.5, stop_words='english',)\n",
        "trainX = vectorize.fit_transform(ng_train.data)\n",
        "\n",
        "print(f'''Train\\t |  {ng_train.target.shape[0]}\\t |  {trainX.shape[1]}\n",
        "Test\\t |  {ng_test.target.shape[0]}\\t |  N/A''')\n",
        "\n",
        "grid.fit(ng_train.data, ng_train.target)\n",
        "print(grid.best_params_)\n",
        "PredY = grid.best_estimator_.predict(ng_test.data)\n",
        "print(metrics.confusion_matrix(ng_test.target, PredY))\n",
        "\n",
        "tree.plot_tree(grid.best_estimator_['clf'], filled=True, class_names=ng_test.target_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set _|_ # Docs  _|_ Attributes\n",
        "\n",
        "Train    |  2148     |  26562\n",
        "\n",
        "Test     |  1430     |  N/A\n",
        "\n",
        "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, \n",
        "'clf__max_leaf_nodes': 20, 'clf__min_samples_leaf': 10, 'clf__min_samples_split': 20}\n",
        "[[192   1 196   0]\n",
        "\n",
        " [ 18 164 214   0]\n",
        "\n",
        " [  4   4 385   1]\n",
        "\n",
        " [  6   8 185  52]]\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dalwVdZhd0Ey"
      },
      "source": [
        "To optimize the large number of parameters Random Search is used and only two options per parameter for Grid search. For parameters that did not change all original options from the list were given and Grid search was run. Optimized parameters were displayed with similar results to Random Search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![<caption>](p2-3-1.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ProjectTemplate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
