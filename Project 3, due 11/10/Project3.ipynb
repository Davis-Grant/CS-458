{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ncDfQIZ5gG"
      },
      "source": [
        "# **Project 3 Report**\n",
        "\n",
        "Grant Davis\n",
        "\n",
        "CS458"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8goBinPaYnd"
      },
      "source": [
        "## **P3-1. Revisit Text Documents**\n",
        "\n",
        "**(a) Develop a decision tree based classifier to classify the 3 different types  of Iris (Setosa, Versicolour, and Virginica).**\n",
        "\n",
        "**(b) Build classifiers using the following methods:**\n",
        "\n",
        "**Support Vector Machine (sklearn.svm.LinearSVC)** \n",
        "\n",
        "**Naive Bayes classifiers (sklearn.naive_bayes.MultinomialNB)**\n",
        "\n",
        "**K-nearest neighbors (sklearn.neighbors.KNeighborsClassifier)** \n",
        "\n",
        "**Random forest (sklearn.ensemble.RandomForestClassifier)** \n",
        "\n",
        "**AdaBoost classifier (sklearn.ensemble.AdaBoostClassifier)** \n",
        "\n",
        "**Optimize the hyperparameters of these methods and compare the results of these methods.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VGfsolCrbInO",
        "outputId": "6ad9aa14-3a19-4410-d01d-801cf0297e49"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets, model_selection, metrics\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "\n",
        "#(a) load newsgroups\n",
        "ng_train = datasets.fetch_20newsgroups(subset='train', categories=['rec.autos', 'talk.religion.misc', 'comp.graphics', 'sci.space'], remove=('headers', 'footers', 'quotes'))\n",
        "ng_test = datasets.fetch_20newsgroups(subset='test', categories=['rec.autos', 'talk.religion.misc', 'comp.graphics', 'sci.space'], remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "y1 = ng_train.target\n",
        "y2 = ng_test.target\n",
        "\n",
        "#(b) classifiers\n",
        "\n",
        "max_depth_range = [None, 2, 5, 10]\n",
        "min_samples_leaf_range = [1, 5, 10]\n",
        "min_sample_split_range = [2, 10, 20]\n",
        "min_leaf_nodes_range = [None, 5, 10, 20]\n",
        "\n",
        "param_grid = {\"clf__criterion\": ['gini'],\n",
        "              \"clf__max_depth\": [10],\n",
        "              \"clf__min_samples_leaf\": [1, 5, 10],\n",
        "              \"clf__min_samples_split\": [20],\n",
        "              \"clf__max_leaf_nodes\": [None, 5, 10, 20]\n",
        "              }\n",
        "\n",
        "pipe_rf = Pipeline([('vect', TfidfVectorizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', RandomForestClassifier())])\n",
        "\n",
        "Results_SVC_Penalty = {\n",
        "  \"l1\" : 0,\n",
        "  \"l2\" : 0\n",
        "}\n",
        "\n",
        "Results_Bayes_Alpha = {\n",
        "  0.001 : 0,\n",
        "  0.01 : 0,\n",
        "  0.1 : 0\n",
        "}\n",
        "\n",
        "Results_KNN_Neighbors = {\n",
        "  5 : 0,\n",
        "  10 : 0,\n",
        "  15 : 0\n",
        "}\n",
        "\n",
        "Results_Ada_LearningRate = {\n",
        "  0.001 : 0,\n",
        "  0.01 : 0,\n",
        "  0.1 : 0,\n",
        "  1.0 : 0\n",
        "}\n",
        "\n",
        "Results_Forest_Multiple = dict()\n",
        "\n",
        "def trainMe(clf, Results, hyperparameter):\n",
        "  print(f\"Testing {str(clf)}\")\n",
        "  clf.fit(x1, y1)\n",
        "  pred = clf.predict(x2)\n",
        "  score = metrics.accuracy_score(y2, pred)\n",
        "  Results[hyperparameter] = score\n",
        "\n",
        "def runTests():\n",
        "  # Support Vector Machine (LinearSVC)\n",
        "  for hp in Results_SVC_Penalty:\n",
        "    trainMe(LinearSVC(penalty=hp, tol=1e-3, dual=False), Results_SVC_Penalty, hp)\n",
        "\n",
        "  # Naive Bayes (MultinomialNB)\n",
        "  for hp in Results_Bayes_Alpha:\n",
        "    trainMe(MultinomialNB(alpha=hp), Results_Bayes_Alpha, hp)\n",
        "\n",
        "  # K-nearest Neighbors (KNeighborsClassifier)\n",
        "  for hp in Results_KNN_Neighbors:\n",
        "    trainMe(KNeighborsClassifier(n_neighbors=hp), Results_KNN_Neighbors, hp)\n",
        "\n",
        "  # Random forest (RandomForestClassifier)\n",
        "  print(\"Testing RandomForestClassifier(*)\")\n",
        "  grid = model_selection.GridSearchCV(estimator=pipe_rf, param_grid=param_grid, scoring='accuracy', refit=True, verbose=True)\n",
        "  grid.fit(ng_train.data, ng_train.target)\n",
        "  means = grid.cv_results_[\"mean_test_score\"]\n",
        "  for mean, params in zip(means, grid.cv_results_[\"params\"]):\n",
        "    Results_Forest_Multiple.update({str(params) : mean})\n",
        "\n",
        "  # AdaBoost (AdaBoostClassifier)\n",
        "  for hp in Results_Ada_LearningRate:\n",
        "    trainMe(AdaBoostClassifier(learning_rate=hp), Results_Ada_LearningRate, hp)\n",
        "  \n",
        "def printDictReallyNice(d):\n",
        "  for k,v in d.items():\n",
        "    print(k, ' : ', v)\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english',)\n",
        "x1 = vectorizer.fit_transform(ng_train.data)\n",
        "x2 = vectorizer.transform(ng_test.data)\n",
        "\n",
        "runTests()\n",
        "print(\"\\nFormat = Hyperparameter : Accuracy\")\n",
        "print(f'\\nSupport Vector Machine\\nHyperparameter: Penalty')\n",
        "printDictReallyNice(Results_SVC_Penalty)\n",
        "print(f'\\nNaive Bayes\\nHyperparameter: Smoothing (alpha)')\n",
        "printDictReallyNice(Results_Bayes_Alpha)\n",
        "print(f'\\nK-nearest Neighbors\\nHyperparameter: Number of Neighbors')\n",
        "printDictReallyNice(Results_KNN_Neighbors)\n",
        "print(f'\\nRandom Forest\\nHyperparameter: Max Depth, Min Samples Leaf, Min Samples Split, Min Leaf Nodes')\n",
        "printDictReallyNice(Results_Forest_Multiple)\n",
        "print(f'\\nAdaBoost Classifier\\nHyperparameter: Learning Rate')\n",
        "printDictReallyNice(Results_Ada_LearningRate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing LinearSVC(dual=False, penalty='l1', tol=0.001)\n",
        "\n",
        "Testing LinearSVC(dual=False, tol=0.001)\n",
        "\n",
        "Testing MultinomialNB(alpha=0.001)\n",
        "\n",
        "Testing MultinomialNB(alpha=0.01)\n",
        "\n",
        "Testing MultinomialNB(alpha=0.1)\n",
        "\n",
        "Testing KNeighborsClassifier()\n",
        "\n",
        "Testing KNeighborsClassifier(n_neighbors=10)\n",
        "\n",
        "Testing KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "Testing RandomForestClassifier(*)\n",
        "\n",
        "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
        "\n",
        "Testing AdaBoostClassifier(learning_rate=0.001)\n",
        "\n",
        "Testing AdaBoostClassifier(learning_rate=0.01)\n",
        "\n",
        "Testing AdaBoostClassifier(learning_rate=0.1)\n",
        "\n",
        "Testing AdaBoostClassifier()\n",
        "\n",
        "Format = Hyperparameter : Accuracy\n",
        "\n",
        "Support Vector Machine\n",
        "\n",
        "Hyperparameter: Penalty\n",
        "\n",
        "l1  :  0.8251748251748252\n",
        "\n",
        "l2  :  0.8748251748251749\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Hyperparameter: Smoothing (alpha)\n",
        "\n",
        "0.001  :  0.8699300699300699\n",
        "\n",
        "0.01  :  0.8748251748251749\n",
        "\n",
        "0.1  :  0.8818181818181818\n",
        "\n",
        "\n",
        "K-nearest Neighbors\n",
        "\n",
        "Hyperparameter: Number of Neighbors\n",
        "\n",
        "5  :  0.28601398601398603\n",
        "\n",
        "10  :  0.2748251748251748\n",
        "\n",
        "15  :  0.26713286713286716\n",
        "\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Hyperparameter: Max Depth, Min Samples Leaf, Min Samples Split, Min Leaf Nodes\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': None, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 20}  :  0.7490724779096871\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': None, 'clf__min_samples_leaf': 5, 'clf__min_samples_split': 20}  :  0.7532650295440994\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': None, 'clf__min_samples_leaf': 10, 'clf__min_samples_split': 20}  :  0.7369664444083048\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': 5, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 20}  :  0.6713167452702337\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': 5, 'clf__min_samples_leaf': 5, 'clf__min_samples_split': 20}  :  0.6917970401691332\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': 5, 'clf__min_samples_leaf': 10, 'clf__min_samples_split': 20}  :  0.6824936304006071\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': 10, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 20}  :  0.727197918360709\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': 10, 'clf__min_samples_leaf': 5, 'clf__min_samples_split': 20}  :  0.7323217867403914\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': 10, 'clf__min_samples_leaf': 10, 'clf__min_samples_split': 20}  :  0.7323315444245677\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': 20, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 20}  :  0.7388355830216295\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': 20, 'clf__min_samples_leaf': 5, 'clf__min_samples_split': 20}  :  0.7430270504689109\n",
        "\n",
        "{'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__max_leaf_nodes': 20, 'clf__min_samples_leaf': 10, 'clf__min_samples_split': 20}  :  0.7337095462676858\n",
        "\n",
        "AdaBoost Classifier\n",
        "\n",
        "Hyperparameter: Learning Rate\n",
        "\n",
        "0.001  :  0.4342657342657343\n",
        "\n",
        "0.01  :  0.4783216783216783\n",
        "\n",
        "0.1  :  0.6601398601398601\n",
        "\n",
        "1.0  :  0.6573426573426573\n",
        "\n",
        "Classifier mehtods SVM, Naive Bayes, and KNN were not too sensitive to the chosen hyperparameters. The Random Forest preferred a higher maximum number of leaf nodes, with around 73% for most runs. AdaBoost, while not very accurate, saw a significant boost when increasing the \"learning rate\" value towards one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vusqyELcnd5"
      },
      "source": [
        "# **P3-2. Recognizing hand-written digits**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbKeiSBCcx64"
      },
      "source": [
        "**(a) Develop a multi-layer perceptron classifier to recognize images of hand-written digits.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m70sWNEMc0a8"
      },
      "source": [
        "**(b) Optimize the hyperparameters of your neural network to maximize the classification accuracy. Show the confusion matrix of your neural network. Discuss and compare your results with the results using a support vector classifier (see https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F5PVO7WkdCAh",
        "outputId": "f2c32dae-0b60-4c19-d490-c29d47f08444"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets, metrics\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# (a) build classifier\n",
        "digits = datasets.load_digits()\n",
        "x1, x2, y1, y2 = train_test_split(digits.data, digits.target, test_size=0.5)\n",
        "good_clf = None\n",
        "\n",
        "class Neural:\n",
        "  def __init__(self, a):\n",
        "    self.alpha = a\n",
        "    self.clf = MLPClassifier(alpha=a)\n",
        "    self.score = 0\n",
        "  \n",
        "  def update(self, s):\n",
        "    self.score = s\n",
        "\n",
        "Results_MLP_Alpha = {\n",
        "  0.0001 : Neural(0.0001),\n",
        "  0.001 : Neural(0.001),\n",
        "  0.01 : Neural(0.01),\n",
        "  0.1 : Neural(0.1)\n",
        "}\n",
        "\n",
        "Results_SVC_Gamma = {\n",
        "  0.0001 : 0,\n",
        "  0.001 : 0,\n",
        "  0.01 : 0,\n",
        "  0.1 : 0\n",
        "}\n",
        "\n",
        "def trainMe(clf, Results, hyperparameter):\n",
        "  print(f\"Testing {str(clf)}\")\n",
        "  clf.fit(x1, y1)\n",
        "  pred = clf.predict(x2)\n",
        "  score = metrics.accuracy_score(y2, pred)\n",
        "  try:\n",
        "    Results[hyperparameter].score = score\n",
        "  except AttributeError:\n",
        "    Results[hyperparameter] = score\n",
        "\n",
        "def runTests():\n",
        "  global good_clf\n",
        "\n",
        "  # MLPClassifier\n",
        "  max_score = 0\n",
        "  for hp in Results_MLP_Alpha:\n",
        "    trainMe(Results_MLP_Alpha[hp].clf, Results_MLP_Alpha, hp)\n",
        "    if Results_MLP_Alpha[hp].score > max_score:\n",
        "      max_score = Results_MLP_Alpha[hp].score\n",
        "      good_clf = Results_MLP_Alpha[hp].clf\n",
        "\n",
        "  # SVC\n",
        "  for hp in Results_SVC_Gamma:\n",
        "    trainMe(SVC(gamma=hp, tol=1e-3), Results_SVC_Gamma, hp)\n",
        "\n",
        "def printDictReallyNice(d):\n",
        "  for k,v in d.items():\n",
        "    try:\n",
        "      print(k, ' : ', v.score)\n",
        "    except AttributeError:\n",
        "      print(k, ' : ', v)\n",
        "\n",
        "runTests()\n",
        "print(\"\\nFormat = Hyperparameter : Accuracy\")\n",
        "print(f'\\nMulti-layer Perceptron\\nHyperparameter: Regularization (alpha)')\n",
        "printDictReallyNice(Results_MLP_Alpha)\n",
        "print(f'\\nSupport Vector Machine\\nHyperparameter: Gamma')\n",
        "printDictReallyNice(Results_SVC_Gamma)\n",
        "print()\n",
        "print(f\"Using {str(good_clf)} as best NN...\\n\")\n",
        "print(\" -- Confusion Matrix of Neural Network --\")\n",
        "print(metrics.confusion_matrix(y2, good_clf.predict(x2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_5NEhCBdXoH"
      },
      "source": [
        "Testing MLPClassifier()\n",
        "\n",
        "Testing MLPClassifier(alpha=0.001)\n",
        "\n",
        "Testing MLPClassifier(alpha=0.01)\n",
        "\n",
        "Testing MLPClassifier(alpha=0.1)\n",
        "\n",
        "Testing SVC(gamma=0.0001)\n",
        "\n",
        "Testing SVC(gamma=0.001)\n",
        "\n",
        "Testing SVC(gamma=0.01)\n",
        "\n",
        "Testing SVC(gamma=0.1)\n",
        "\n",
        "\n",
        "Format = Hyperparameter : Accuracy\n",
        "\n",
        "\n",
        "Multi-layer Perceptron\n",
        "\n",
        "Hyperparameter: Regularization (alpha)\n",
        "\n",
        "0.0001  :  0.9710789766407119\n",
        "\n",
        "0.001  :  0.9655172413793104\n",
        "\n",
        "0.01  :  0.967741935483871\n",
        "\n",
        "0.1  :  0.9666295884315906\n",
        "\n",
        "\n",
        "Support Vector Machine\n",
        "\n",
        "Hyperparameter: Gamma\n",
        "\n",
        "0.0001  :  0.9655172413793104\n",
        "\n",
        "0.001  :  0.9888765294771968\n",
        "\n",
        "0.01  :  0.7063403781979978\n",
        "\n",
        "0.1  :  0.08898776418242492\n",
        "\n",
        "\n",
        "Using MLPClassifier() as best NN...\n",
        "\n",
        "\n",
        " -- Confusion Matrix of Neural Network --\n",
        "\n",
        "[[88  0  0  0  1  0  0  0  0  0]\n",
        "\n",
        " [ 1 77  1  1  0  0  0  0  0  0]\n",
        "\n",
        " [ 0  0 97  0  0  0  0  0  0  0]\n",
        "\n",
        " [ 0  0  0 83  0  3  0  0  0  0]\n",
        "\n",
        " [ 1  1  0  0 91  0  0  0  1  0]\n",
        "\n",
        " [ 0  1  0  0  0 99  0  0  0  0]\n",
        "\n",
        " [ 0  0  0  0  2  0 92  0  0  0]\n",
        "\n",
        " [ 0  0  0  0  2  0  0 85  0  0]\n",
        "\n",
        " [ 0  5  0  0  0  2  0  0 75  0]\n",
        "\n",
        " [ 0  0  0  0  0  0  0  3  1 86]]\n",
        " \n",
        "The multi-layer perceptron is the better classifier for this situation. Accuracy is around 96% after adjusting hyperparameter for regularization. Meanwhile, the Support Vector Machine is accurate at lower gamma values but loses accuracy significantly at higher values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot87gIvndb4b"
      },
      "source": [
        "# **P3-3. Nonlinear Support Vector Machine**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcX7qMj9dfLm"
      },
      "source": [
        "**(a) Randomly generate the following 2-class data points.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(b) Develop a nonlinear SVM binary classifier (sklern.svm.NuSVC).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(c) Plot these data points and the corresponding decision boundaries, which is similar to the figure in the slide 131 in Chapter 4.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import NuSVC\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#(a) generate 2-class data\n",
        "np.random.seed(0)\n",
        "x = np.random.rand(300,2) * 10 - 5\n",
        "y = np.logical_xor(x[:,0]>0, x[:,1]>0)\n",
        "\n",
        "#(b) develop nonlinear SVM binary classifier\n",
        "clf = NuSVC(gamma='auto')\n",
        "clf.fit(x,y)\n",
        "\n",
        "#(c) plot decision boundaries\n",
        "xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))\n",
        "z = clf.decision_function(np.c_[xx.ravel(),yy.ravel()])\n",
        "z = z.reshape(xx.shape)\n",
        "\n",
        "cont = plt.contour(xx, yy, z, linewidths=2, levels=[0])\n",
        "plt.scatter(x[y == 0,0], x[y == 0,1], marker='o', cmap=plt.cm.bwr, edgecolors='k')\n",
        "plt.scatter(x[y == 1,0], x[y == 1,1], marker='s', cmap=plt.cm.bwr, edgecolors='k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![<caption>](pic123.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This graph shows the classifier is fairly acccurate with its decision boundaries, with only a few points that are misclassified."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ProjectTemplate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "7ccada5d18996c10de7294fc0baea8bf73b5d2e1b7030203c312a5b4ac0b0c5a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
